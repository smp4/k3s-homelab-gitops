{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"K3S Homelab Gitops Documentation","text":"<p>A basic, unavoidably opinionated, non-professional template for bootstrapping a K3S cluster at home using a GitOps approach.</p> <p>This repo is a basic, unavoidably opinionated, non-professional template for bootstrapping a K3S cluster at home using a GitOps approach.</p> <p>This is the public version of the GitOps infrastructure repository for my home cluster. It is intended to be used as a template for a new cluster, and then to be forked and modified to suit your needs. </p> <p> Use at your own risk! This template attempts, but does not guarantee, to follow configuration and security best practices. It was not created by a kubernetes professional. I am not responsible for any damages or data loss incurred by your usage of this template. </p> <p>Head over to Getting Started.</p>"},{"location":"development/","title":"Development","text":"<p>All requirements for bootstrapping, docs and development can be installed from the single <code>requirements.txt</code> file. All assisting infrastructure is Python-based. Install with</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"development/#pre-commit","title":"Pre-commit","text":"<p>As a precaution against accidental secret leaks, a gitleaks pre-commit hook is provided. Pre-commit will be installed when you install from <code>requirements.txt</code>.</p> <p>Continue the first-time setup for the hooks with the following:</p> <pre><code>pre-commit autoupdate  # update the pre-commit hook versions\npre-commit install  # install the hooks\n</code></pre> <p>The gitleaks hook will scan staged, but not yet committed, files for secrets. The commit will fail if it finds a leaked secret that you are inadvertently trying to commit.</p> <p>To modify either the pre-commit hook or gitleaks, see their respective documentation.</p>"},{"location":"development/#publishing-the-docs","title":"Publishing the docs","text":"<p>The documents are built using Material for Mkdocs.</p> <p>To preview the docs as you edit:</p> <pre><code>mkdocs serve\n</code></pre> <p>To check the build:</p> <pre><code>mkdocs build --strict\n</code></pre> <p>The docs are published to GitHub pages with GitHub Actions.</p>"},{"location":"development/#updating-the-changelog","title":"Updating the changelog","text":"<p>The Changelog is maintained with scriv.</p> <p>Add a changelog fragment with every fix, change, feature addition etc:</p> <pre><code>scriv create\n</code></pre> <p>The fragment will go to the <code>changelog.d</code> directory and you can edit it there.</p> <p>Upon release, set the desired version number in <code>changelog.d/scriv.ini</code> and compile the fragments into the next version:</p> <pre><code>scriv collect\n</code></pre> <p>The fragments will be added to the <code>CHANGELOG.md</code> file in the root directory. It can then be cleaned up manually before committing. </p>"},{"location":"development/#github-actions","title":"Github Actions","text":"<p>Two actions are pre-configured:</p> <ul> <li>Auto-update pre-commit hook versions</li> <li>Build the docs and publish to GitHub Pages </li> </ul> <p>For the pre-commit action to work, you must give GitHub Actions permission to create pull requests to your repo. You can find this setting in your repository settings under Actions - General - Workflow permissions.</p>"},{"location":"about/CHANGELOG/","title":"Changelog","text":"<p>This is the changelog for the K3s Homelab Gitops repository. All enhancements and patches will be documented in this file. It adheres to the structure of Keep a Changelog v1.1.0.</p> <p>This project adheres to Semantic Versioning.</p>"},{"location":"about/CHANGELOG/#unreleased","title":"Unreleased","text":"<p>See the fragment files in the <code>changelog.d</code> directory.</p>"},{"location":"explanations/","title":"Explanations","text":"<p>The explanations provide more in depth descriptions and justifications of why this template does what it does.</p>"},{"location":"explanations/cluster-organisation/","title":"Cluster organisation and GitOps approach","text":""},{"location":"explanations/cluster-organisation/#cluster","title":"Cluster","text":"<p>The GitOps approach assumes a single cluster and that ArgoCD and all managed applications are installed inside the same cluster. It will be one large shared cluster acting like a general purpose infrastructure platform for homelab workloads.</p> <p>On the plus side, this means only one copy of the cluster infrastructure (load balancer, ingress, monitoring etc) is needed and can be shared by all workloads. It is also easier to administrate than having multiple clusters for each app.</p> <p>Downsides of this architecture include the risk that non-prod instances can starve prod instances of resources, poorer security, risk of non-prod config errors impacting prod....etc. See learning k8s.</p>"},{"location":"explanations/cluster-organisation/#environments","title":"Environments","text":"<p>Kustomize is used to create <code>base</code> workload configurations with environment-specific modifications stored in individual overlay directories per environment. The repo does not use branching for different environments. </p> <p>To activate an environment for a workload (which will be deployed as an ArgoCD Application), simply include a <code>config.json</code> file in the desired environment directory.</p>"},{"location":"explanations/cluster-organisation/#argocd-projects","title":"ArgoCD Projects","text":"<p>With the exception of the <code>kube-system</code> namespace, cluster workloads are entirely managed by ArgoCD. ArgoCD organises namespaces into projects. From the ArgoCD docs:</p> <p>Projects provide the following features:</p> <ul> <li>restrict what may be deployed (trusted Git source repositories)</li> <li>restrict where apps may be deployed to (destination clusters and namespaces)</li> <li>restrict what kinds of objects may or may not be deployed (e.g. RBAC, CRDs, DaemonSets, NetworkPolicy etc...)</li> <li>defining project roles to provide application RBAC (bound to OIDC groups and/or JWT tokens)</li> </ul> <p>Additional projects can be created to give separate teams different levels of access to namespaces.</p> <p>If unspecified, an application belongs to the default project, which is created automatically and by default, permits deployments from any source repo, to any cluster, and all resource Kinds. The default project can be modified, but not deleted.</p> <p>Note:</p> <p>Projects with access to the namespace in which Argo CD is installed effectively have admin-level privileges.</p>"},{"location":"explanations/cluster-organisation/#argocd-applications","title":"ArgoCD Applications","text":"<p>An application may consistent of multiple components (ie. separate containers for frontend, backend, database), managed as (possibly) an App of Apps pattern via ArgoCD, and deployments of that application into each of its required environments constitute discrete, separate instances of that application. That is, the number of application instances is the number of apps multiplied by the number of (activated) environments within which those apps are to be deployed.</p>"},{"location":"explanations/cluster-organisation/#separating-teams-with-kubernetes-objects","title":"Separating teams with Kubernetes objects","text":"<p>For a single user, most of the below is unnecessary, but it has been implemented for practice.</p> <ul> <li>ArgoCD AppProjects delineate which users can deploy what, where (even if in this homelab case, there is only one user). The Ops team uses the default ArgoCD project, <code>default</code>, which permits all source repos, namespaces and destinations. The assumption here is that the Ops team are admins and they know what they're doing. Cluster infrastructure is deployed within this project.</li> <li>Developers receive their own ArgoCD AppProject(s). The cluster is bootstrapped with a single AppProject, <code>dev1</code>. This restricts the source repo to the <code>tenants</code> directory of the GitOps repo and deployments to the local cluster only. Additionally, <code>kube-system</code> and <code>argocd</code> namespaces are denied. This should support \"dev team self service\". See the ArgoCD example.</li> <li>Environments are created per app instance and each instance has its own namespace, for example <code>myapp-stage</code>, to provide logical separation within the cluster.</li> <li>Operations teams to easily deploy cluster infrastructure add-ons using the <code>infrastructure</code> ApplicationSet. </li> </ul>"},{"location":"explanations/cluster-organisation/#notes-on-security","title":"Notes on security","text":"<p>Note that using namespaces for each app instance provides logical separation only, namespaces do not provide isolation and security between Kubernetes resources. Unless you take active steps against it, all communication inside a Kubernetes cluster is allowed by default. And contrary to popular belief a pod from one namespace can freely communicate with a pod on another namespace.. Kubernetes namespaces are not a security measure.</p> <p>Furthermore, ensure that only admins have the right to write/ merge into the cluster GitOps repo, to avoid accidental or malicious deployments. See ArgoCD docs for more. For further access controls use dev self-service with ApplicationSets, see Appset in any namespace.</p> <p>For homelab use, I am the only person with admin access to the nodes, cluster control plane APIs and GitOps repo, so the <code>dev1</code> project is set up simply to make sure tenant workloads are not inadvertently deployed to the wrong cluster or admin-level namespaces.</p>"},{"location":"explanations/cluster-organisation/#gitops-repo-directory-structure","title":"GitOps repo directory structure","text":"<p>The repo structure attempts to merge guidance from Codefresh on modelling environments and Red Hat on directory structure best practice. It should reflect a monorepo approach where Ops and Dev both work together in the same GitOps repository.</p> <ul> <li>Ops deployments (cluster infrastructure) reside in <code>infrastructure</code>, developer deployments (apps) reside in <code>tenants</code>.</li> <li><code>components</code> are cluster resources defined and owned by Ops. These are Kubernetes specific settings for infrastructure and applications, such as namespaces, replicas, resource limits, health checks, persistent volumes, affinity rules etc.</li> <li>Each application contains a <code>base</code> directory containing configuration common to all instances of that workload across all of its environments. It should not change often. This is where environment configuration is mostly static business settings. These are settings that having nothing to do with Kubernetes, they never get promoted between environment types (ie. never from stage to prod), and are applicable to all environments of a given type on this cluster (ie. applicable to every staging environment used by an app). Things like the URLs that a production environment should use. Some or all of the structure of this directory may be replicated with an environment directory to apply environment configuration specific to that workload. </li> <li>The <code>bootstrap</code> directory contains everything needed (and no more) to initialise the cluster. Ansible provisions the bare cluster and bootstraps the GitOps controller (ArgoCD) from the resources in this directory. </li> </ul> <p>Extensions (not implemented):</p> <ul> <li>Use a <code>variants</code> directory to define settings common to a certain type of environment (eg. all <code>stage</code> environments) or other aspect common between multiple environments. An example might be authentication values for a validation database to be used in non-production environments. When the application instance is deployed, it will inherit both cluster-wide and app-specific configuration for its environment instance from these directories.</li> </ul>"},{"location":"explanations/cluster-organisation/#infrastructure-applications","title":"Infrastructure applications","text":"<p>The level 1 directory <code>infrastructure</code> (the repository root directory is level 0) contains infrastructure applications that will be shared by all applications deployed to the cluster. The <code>infrastructure-appset</code> automatically deploys each app with a level 2 directory inside <code>infrastructure</code>.</p> <p>Only one instance of an infrastructure application should ever be deployed, no matter whether that instance is in a qa, dev, stage or prod environment. Therefore level 4 environment directories (for example <code>infrastructure/my-infra-app/envs/level-4-env-dir</code>) are not used.</p> <p>There is only ever one namespace per infrastructure app, named after the application name, eg. <code>my-infra-app</code>.</p>"},{"location":"explanations/cluster-organisation/#tenant-applications","title":"Tenant applications","text":"<p>Tenant applications reside in the <code>tenants</code> level 1 directory. By default, the <code>tenants-appset</code> automatically deploys each app with a level 2 directory inside <code>tenants</code>. Other ApplicationSet objects may be created in the future by ops if needed, to deploy app with different configuration.</p> <p>In contrast to infrastructure application behaviour, tenant applications can have any number of environments defined and deployed simultaneously. Each app-environment instance will have its own namespace <code>&lt;myapp&gt;-&lt;env&gt;</code>. </p> <p>In theory, namespaces should only be defined by ops in <code>components</code>, which means the required namespaces must be defined by ops before dev sets up the app deployment configuration in <code>dev</code>. The template does not currently implement this.</p> <p>Two example tenant applications are provided by default, to verify the ingress and the load balancer.</p>"},{"location":"explanations/cluster-organisation/#templating-vs-patching","title":"Templating vs patching","text":"<p>As summarised by Red Hat, a key objective of setting up the GitOps repo is to keep it DRY (Don't Repeat Yourself), aka have a single source of truth. This means using Kustomize and/ or Helm for building up the YAML configuration files.</p> <p>Kustomize uses a patching method which is best when the operator already knows the configurations and deltas beforehand. When you don't know the values beforehand, Helm works much better. Helm uses a templating method that makes it easier to change a variable in many places at once you know what it should be.</p> <p>Both methods are useful and the repo should probably use both tools as needed (which it does). There is no rule that says only one tool should be used.</p> <p>An alternative for bootstrapping can be to use Ansible to generate the initial Kustomize YAML from Ansible templates. The readability of the resulting repo is pretty poor, requiring the operator to either do a dummy run of the Ansible playbook to generate the Kustomize yaml to see what it would look like, or try to infer it from the Ansible templates. Not to mention that there are now two tools working together, where perhaps one might have been better (Helm). Examples are here and here.</p> <p>The ArgoCD ApplicationSet object also allows a form of templating.</p> <p>For general ease of use, it is common to use Helm for third party workloads that others have pre-packaged, and Kustomize for your own custom workloads.</p> <p>For several of the infrastructure workloads, Helm and Kustomize are used together. The Helm Chart for the workload is \"hydrated\" by Kustomize, allowing the easy modification of the generic Chart via the available Helm values, and then overlaid with further environment specific modifications as necessary with Kustomize. See the Traefik workload, for example.</p>"},{"location":"explanations/naming/","title":"Naming conventions","text":"<p>This repo uses the following file, directory and kubernetes object naming conventions:-</p> <ul> <li>Directory names are normally one word generically describing the kubernetes objects, or environment, or type of configuration they contain.</li> <li>YAML configuration filenames are of the form <code>&lt;k8s_object_type_abbreviation&gt;-&lt;description&gt;.yaml</code>. The description should be terse and without hyphens. Use underscores in the description if needed for readability, but the preference is to do without any delimiters in the description.</li> <li><code>pv-&lt;description&gt;.yaml</code></li> <li><code>pvc-&lt;description&gt;.yaml</code></li> <li><code>appproj-&lt;description&gt;.yaml</code></li> <li><code>appset-&lt;description&gt;.yaml</code></li> <li>...</li> <li>Kubernetes object names (stated within the configuration files) should be of the form <code>&lt;description&gt;-&lt;k8s_object_type_abbreviation&gt;</code>. Hyphens are allowed for readability. For example, <code>argo-gitops-pv</code>.</li> <li><code>&lt;description&gt;-pv</code></li> <li><code>&lt;description&gt;-pvc</code></li> <li><code>&lt;description&gt;-appproj</code></li> <li><code>&lt;description&gt;-appset</code></li> <li>...</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>If you're just getting started with a kubernetes homelab cluster, follow the Installation instructions to get all the requirements set up. This will get you into a position ready to deploy a single node k3s cluster to your local machine.</p> <p>Once you've got all of the requirements working locally, move on to the first tutorial to deploy a development K3s instance to your cluster. The tutorials then progressively upgrade the cluster to staging and production environments.</p> <p>Note</p> <p>The progressive rollout is important and unavoidable. The initial cluster bootstrap requires the GitOps repo to be available at a location that does not require secrets to access, so we use a local git repo on one of the cluster nodes. Only once we can generate and encrypt secrets (which requires the cluster to be running), can we then migrate to a remotely hosted GitOps repo (eg. on GitHub).</p> <p>Additionally, the cluster uses Let's Encrypt for SSL certificates and if you deploy directly to this repo's production environment, you will be using the Let's Encrypt production endpoint. If you hit that endpoint too frequently (very likely if you don't know what you're doing), they will rate limit you and you'll have to wait a few days to use their prod endpoint again.</p> <p>Many of the directories in the repo have their own <code>README.md</code> files providing further details on how to interpret and use their contents. </p> <p>So - go get started! See the Installation docs. </p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This page describes how to set up and install this Git repo on your local machine including getting the repo, installing Ansible, generating secrets and configuring the Ansible inventory and variables.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>At least one machine to act as a server node.</li> <li>Operating on a UNIX/ Linux operating system.</li> <li>Minimum 1, ideally 3, machines or VM's to host the cluster nodes. </li> <li>You must have <code>git</code> installed on at least your local workstation and the master server node.</li> <li>You have set up SSH access to all cluster nodes, ideally password-less.</li> <li>All cluster machines must have the relevant ports open on their firewalls. See below.</li> </ul> <p>Preferably:</p> <ul> <li>A domain name to use for the cluster, with DNS and Let's Encrypt set up to issue certificates. See Techno Tim for a guide.</li> </ul> <p>Info</p> <p>HA ArgoCD is installed by default, which requires at least three nodes. If your cluster has less than three nodes, the HA installation will still work, however you will have several ArgoCD pods stuck in the <code>Pending</code> state. This won't affect Argo but will chew up node resources. If using less than three nodes, change the ArgoCD <code>install.yaml</code> location specification in <code>bootstrap/base/kustomization.yaml</code>.</p> <p>See k3s requirements docs. When installing with multiple server nodes, the Ansible scripts will install K3S with the <code>--cluster-init</code> flag, which will set up an embedded <code>etcd</code> datastore. This requires further ports to be opened on the servers. Again, see the K3S docs.</p> <p>Note, the following installation instructions will overwrite existing K3s installations on the hosts, and will overwrite the <code>.kube</code> directory on the hosts, if it exists.</p>"},{"location":"getting-started/installation/#ports-and-firewall-rules","title":"Ports and firewall rules","text":"<p>For this cluster to work, make sure the following ports are open. For more information, see K3s docs.</p> Protocol Port Source Destination Description TCP 2379 - 2380 Servers Servers HA K3s with embedded etcd TCP 6443 Agents and workstation Servers K3s supervisor and Kubernetes API Server UDP 8472 All nodes All nodes Flannel VXLAN TCP 10250 All nodes All nodes Kubelet metrics TCP &amp; UDP 7946 All nodes and clients Servers MetalLB L2 mode traffic TCP 7472 All nodes All nodes MetalLB metrics between nodes 10.42.0.0/16 any K3s pods 10.43.0.0/16 any K3s services <p>Note also that if you are using an NFS network store as backup target, all nodes will need to have access to that target, as will the K3s pods (<code>10.42.0.0/16</code>).</p> <p>Example NFS setup:</p> Protocol Port Source Destination Description 2049 10.42.0.0/16 NFS target host K3s pods to NFS 111 10.43.0.0/16 NFS target host K3s pods to NFS 2049 Servers NFS target host K3s server nodes running Longhorn to NFS"},{"location":"getting-started/installation/#definitions","title":"Definitions","text":"<p>Before going further, some definitions are necessary.</p> <ul> <li>Site: Refers to your infrastructure - the totality of the local (and possibly remote) machines and connecting network that will constitute your cluster and client machines connecting to your cluster. This term is relevant only within the scope of this repo.</li> <li>Local machine: Your local machine can take on any of the above roles. In fact, with K3s, it can simultaneously take all three if you wish. Also called your local host.</li> <li>Ansible controller workstation: The machine from which you will be executing any Ansible playbooks or Makefile commands from this repo. It's most likely your local machine and probably not a cluster node. It is not necessary that it isn't a cluster node, but you might find interesting (read: painful) edge cases if your local machine is also a cluster node.</li> <li>K3s/ Kubernetes server nodes: One or more machines running K3s as server nodes. The server nodes run together to form the cluster control plane. Given how voting amongst server nodes works, it makes sense for the control plane to consist of an odd number of server nodes only. A minimum of one server node is necessary to have a K3s cluster, in which case that node will operate as the server as well as execute workloads. A Highly Available (HA) control plane will exist only once there are three or more (5, 7, ...) server nodes.</li> <li>Master server node: The first machine in the <code>server</code> list of the Ansible inventory. This term is relevant only within the scope of this repo. While all K3s server nodes are created equal, the Ansible playbooks in this repo will provision and bootstrap the cluster infrastructure workload to the master server node first, before adding other server or agent nodes to the cluster. This is because the virtual IP provided by KubeVIP needs to be available before any other nodes can be added, and this will only happen once KubeVIP is deployed to the cluster.</li> <li>K3s agent nodes: One or more machines running K3s as an agent. These are worker nodes that execute workloads. A server node can also be a worker node. Any number of agent nodes can be added to the cluster, to increase the available resources and redundancy.</li> </ul> <p>All machines are assumed to be on the same private network.</p>"},{"location":"getting-started/installation/#install-kubectl-argocd-cli-and-helm","title":"Install kubectl, ArgoCD CLI and Helm","text":"<p>Warning</p> <p>You must use a kubectl version that is within one minor version difference of your cluster. For example, a v1.29 client can communicate with v1.28, v1.29, and v1.30 control planes. Using the latest compatible version of kubectl helps avoid unforeseen issues.</p> <p>Install <code>kubectl</code> following the docs. <code>kustomize</code> comes as part of <code>kubectl</code>, albeit an earlier version sometimes. </p> <p>Install <code>helm</code> following the docs.</p> <p>Install ArgoCD CLI following the docs.</p>"},{"location":"getting-started/installation/#initialise-gitops-repo","title":"Initialise GitOps repo","text":"<p>This is a public repo. Ultimately, you will want to be using this as a private repo, but perhaps you will want to keep it connected to this public instance to be able to pull upstream changes. To achieve this, mirror this public repo into your own private repo.  </p> <p>First, get a copy of this repo. The following assumes you want to take this template as a starting point to create a new repo (eg. your private GitOps repo), then make your own modifications, while still retaining the ability to pull updates from the template. Reference SO.</p> <p>Create the private repo in gitlab. In whichever directory you store your projects:</p> <pre><code># clone the template to your private repo\ngit clone git@github.com:smp4/k3s-homelab-gitops.git k3s-homelab-gitops-private\ncd k3s-homelab-gitops-private\n\n# set the remote origin to your private repo on GitHub\ngit remote set-url origin git@github.com:YOURUSERNAME/k3s-homelab-gitops-private.git\n\n# add the public template repo on GitHub as an upstream source\ngit remote add upstream git@github.com:smp4/k3s-homelab-gitops.git\ngit remote -v show  # verify\n\n# push new repo to GitHub\ngit push -u origin main\n\n# do work, commit, push to private repo\ngit push origin main\n\n# pull updates from the public template\ngit fetch upstream\ngit merge upstream/main\n</code></pre> <p>Note</p> <p>This is not a fork. Create a fork if you want to contribute back to the public template.</p>"},{"location":"getting-started/installation/#pulling-updates-from-public-repo","title":"Pulling updates from public repo","text":"<p>In your private repo, pull the updates from the public repo, then pull the latest refs from your private repo's remote, then push to your private repo's remote.</p> <pre><code>git pull --no-rebase upstream main\ngit pull origin main\ngit push origin main\n</code></pre>"},{"location":"getting-started/installation/#install-ansible","title":"Install Ansible","text":"<p>Ansible is used for setup operations on the cluster node machines - OS level tasks.</p> <p>For the most part, the repo does not use Ansible to do any templating of the Kubernetes resource yaml files. The idea is that Ansible is used only at bootstrap and then forgotten. An exception is some of the cluster infrastructure resources that are applied at bootstrap, including Kube VIP, where things like <code>apiserver_endpoint</code> are templated into the Kube VIP manifest.</p> <p>Install Ansible in a Python virtual environment on the Ansible controller workstation (Ansible is a Python application).</p> <p>Optionally use <code>direnv</code> to automatically load environment variables and activate the Python virtual environment whenever you <code>cd</code> into the repo directory.</p> <p>Install <code>direnv</code>:</p> <pre><code>curl -sfL https://direnv.net/install.sh | bash\n</code></pre> <p>Add the following line to <code>~/.zshrc</code> (see the <code>direnv</code> docs for other shells):</p> <pre><code>eval \"$(direnv hook zsh)\"\n</code></pre> <p>Create <code>.envrc</code> file in the root directory of this repo and set up Python virtual environment, specifying <code>.venv</code> as the directory to store the virtual environment (so that it is easily recognised by IDE's), and explicitly selecting a Python version:</p> <pre><code>echo \"export VIRTUAL_ENV=.venv\" &gt;&gt; .envrc\necho \"layout python /usr/local/bin/python3.11\" &gt;&gt; .envrc\ndirenv allow\nwhich python3\nwhich pip\npip install --upgrade pip\n</code></pre> <p>Otherwise, create the virtual environment using your preferred method.</p> <p>With the virtual environment created, install requirements:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>If needed in development, use a <code>.env</code> file to define environment variables within the repo.</p> <p>Install the required Ansible collections:</p> <pre><code>ansible-galaxy install -r ./ansible/collections/requirements.yml\n</code></pre>"},{"location":"getting-started/installation/#create-ansible-inventory-and-values-files","title":"Create Ansible inventory and values files","text":"<p>Next, configure your cluster site. If you are going to follow the tutorials and first deploy to your local host, you will need to populate the <code>ansible/inventory/localhost.yml</code> file. If you are only following the other tutorials, or deploying directly to a cluster of nodes, then you can ignore this file. All users will eventually need to populate the <code>ansible/inventory/hosts.yml</code> file.</p> <p>Start with the defaults in the <code>samples/</code> directory:</p> <pre><code>cp ./samples/localhost-sample.yml ./ansible/inventory/localhost.yml\ncp ./samples/hosts-sample.yml ./ansible/inventory/hosts.yml\ncp ./samples/all-sample.yml ./ansible/inventory/group_vars/all.yml\n</code></pre> <p>Edit <code>localhost.yml</code> and <code>all.yml</code> with the relevant values for your local machine. Descriptions for each variable are given in the sample files. The <code>localhost.yml</code>, make sure you list connection details for the local machine in the <code>servers</code> group only, and no machines in the <code>agent</code> group.</p> <p>Update the host connection details in <code>hosts.yml</code> and cluster configuration variables in <code>all.yml</code> to suit your needs. You may need to modify the <code>all.yml</code> file when progressing to cluster deployment. </p> <p>At this point, it might be useful to understand what each of the directories in the repo contents are doing. </p>"},{"location":"getting-started/installation/#generate-templated-manifests","title":"Generate templated manifests","text":"<p>Ansible is mostly used for bootstrapping the cluster nodes, however in a small number of cases it is mandatory to generate some kubernetes manifest (yaml) files from templates. This is just a hack to use Ansible to template and generate the KubeVIP and MetalLB manifests into the working directory of the GitOps repo on your local machine and only needs to be performed once prior to the first cluster deployment. </p> <p>Warning</p> <p>The following steps generating the KubeVIP templates and committing them to the repo must be completed before any deployments are made to the cluster, as KubeVIP creates the API endpoint IP address which both your local machine and all cluster nodes require to connect to and join the cluster.</p> <p>Create the KubeVIP and MetalLB manifests from the Ansible templates with the Ansible playbook <code>generate-templates</code>. From the repo root directory:</p> <pre><code>make manifest-templates\n</code></pre> <p>It does not matter which Ansible inventory file is used to execute the <code>generate-templates</code> play, as it will execute it on the local machine (the Ansible Controller) only, and once only. The contents of the values file <code>all.yml</code> are the main input. </p> <p>Warning</p> <p>The <code>make manifest-templates</code> command must be run from the root directory of this repo (the directory in which you found the <code>README.md</code> that you're currently reading). This directory is used to template and copy the kubeVIP manifests into the respective infrastructure workload directories in <code>./infrastructure</code>.</p> <p>Commit the new files and push to the repository so that ArgoCD can reconcile them into the cluster later.</p> <pre><code>git status\ngit add .\ngit commit -m \"Add KubeVIP manifests.\"\ngit push  # Assuming the repo is already set up to push to remote origin.\n</code></pre>"},{"location":"getting-started/installation/#create-local-bootstrap-secrets","title":"Create local bootstrap secrets","text":"<p>These secrets are used by Ansible to bootstrap the node machines. Encrypted secrets are implemented as variables (rather than files).</p> <p>Bootstrap secrets (host <code>ansible_become_pass</code>, <code>k3s_token</code> ) are stored in Ansible Vault. Ansible Vault comes as part of the Ansible installation. The user must create these secrets locally, and store them locally. They are never used again once the cluster is initialised. The encrypted secrets are provided to Ansible via <code>ansible/inventory/group_vars/all.yml</code> and <code>ansible/inventory/hosts.yml</code> (or <code>localhost.yml</code>). These files, with their secrets, are listed in <code>.gitignore</code>, so are never committed to version control. </p> <p>Production-time secrets will be separately encrypted and stored using Sealed Secrets.</p> <p>The user must create the bootstrap encrypted secrets first, before running any Ansible playbooks. The scripts are currently configured assuming all secrets belong to a <code>vault-id</code> called <code>home</code>, encrypted with a single password.</p> <p>Create the secrets at the prompts triggered by each of the following commands. Don't hit <code>enter</code> after entering the password: use <code>ctrl-d</code> per the instructions that Ansible will print to screen. Use the same vault password for each command (you can use different passwords if you want, but then the <code>make</code> commands in future steps won't work out of hte box).</p> <p>To encrypt a password to elevate <code>ansible_user</code> privileges on a host, run and paste the output of the following for the respective host in <code>hosts.yml</code>:</p> <pre><code>ansible-vault encrypt_string --vault-id home@prompt --stdin-name \"ansible_become_pass\"\n</code></pre> <p>To encrypt the <code>vault_k3s_token</code> variable in <code>all.yml</code>:</p> <pre><code>ansible-vault encrypt_string --vault-id home@prompt --stdin-name \"k3s_token\"\n</code></pre> <p>The above secrets need to be generated for each node.</p> <p>For convenience, save your vault password in plain text in a file called <code>vault_pass</code> in the root directory of this repo. Make sure this filename is in your <code>.gitignore</code> so that it doesn't get tracked by Git. </p> <pre><code># example vault_pass file\nyour-password-here-in-plain-text\n</code></pre> <p>Some of the Ansible-related <code>make</code> commands used throughout the tutorials will call this password automatically to decrypt your secrets. If you don't want to store your password like this, you will need to manually edit the <code>Makefile</code> yourself. Other commands will prompt the user for the password of the <code>home</code> vault. Use these as an example for your changes.</p> <p>Now <code>kubectl</code> can be used without having to use <code>sudo</code> or manually specify the cluster config location.</p> <p>From here, if you know what you are doing you can provision and deploy the cluster directly to production. To take an incremental approach, checking that everything works at each step, deploy in stages following the Tutorials.</p>"},{"location":"getting-started/installation/#customise-the-cluster-configuration","title":"Customise the cluster configuration","text":"<p>Manually edit the following <code>*.yaml</code> files to suit your preferences.</p> File Key Value Comment <code>bootstrap/base/ingress-argo.yaml</code> <code>spec.routes.match:</code> <code>Host('argocd.your.domain.com')</code> URL for ArgoCD UI <code>components/envs/prod/patch-appproj-dev1-sourceRepos.yaml</code> <code>spec.sourceRepos:</code> Private Gitops Repo URL (ssh) <code>components/envs/prod/patch-appset-infrastructure-generators.yaml</code> <code>spec.generators.git.repoURL:</code> Private Gitops Repo URL (ssh) <code>components/envs/prod/patch-appset-infrastructure-source.yaml</code> <code>spec.template.spec.source.repoURL:</code> Private Gitops Repo URL (ssh) <code>components/envs/prod/patch-appset-tenants-generators.yaml</code> <code>spec.generators.git.repoURL:</code> Private Gitops Repo URL (ssh) <code>components/envs/prod/patch-appset-tenants-source.yaml</code> <code>spec.template.spec.source.repoURL</code> Private Gitops Repo URL (ssh) <code>infrastructure/cert-manager/base/issuer-letsencrypt-prod.yaml</code> <code>spec.acme.email</code>, <code>spec.acme.solvers.dns01.cloudflare.email</code> Your DNS service email address (eg Cloudflare) <code>infrastructure/cert-manager/base/issuer-letsencrypt-prod.yaml</code> <code>spec.solvers.selector.dnsZones</code> Your DNS zone URL <code>infrastructure/cert-manager/base/issuer-letsencrypt-stage.yaml</code> per prod per prod <code>infrastructure/longhorn/base/ingressRoute-dashboard.yaml</code> <code>spec.routes.match:</code> <code>Host('longhorn.your.domain.com')</code> URL for longhorn UI <code>infrastructure/longhorn/base/setting-buTarget.yaml</code> <code>value:</code> <code>nfs://192.168.0.1:/path/to/your/nfs/backup/target</code> Same directory as set in Ansible <code>all.yml</code> values file for <code>longhorn_nfs_backup_target</code> <code>infrastructure/traefik/base/ingress-dashboard.yaml</code> <code>spec.routes.match:</code> Host('traefik.your.domain.com')` URL for Traefik UI <code>infrastructure/traefik/base/values-traefik.yaml</code> <code>loadBalncerIP:</code> Any IP in the MetalLB range <code>infrastructure/traefik/envs/dev/cert-selfsigned.yaml</code> <code>spec.commonName:</code>, <code>spec.dnsNames:</code> <code>traefik.your.domain.com</code> URL for Traefik UI <code>infrastructure/traefik/envs/prod/cert-wildcard-prod.yaml</code> <code>spec.dnsNames:</code>, <code>spec.commonName</code> <code>your.domain.come</code> Your root domain URL <code>infrastructure/traefik/envs/stage/cert-wildcard-prod.yaml</code> <code>spec.dnsNames:</code>, <code>spec.commonName</code> <code>your.domain.come</code> Your root domain URL <code>tenants/test-ingress/base/whoami.yaml</code> In IngressRoute <code>spec.routes.match:</code> <code>Host('test.your.domain.com)</code> URL for ingress test tenant workload <code>tenants/test-lb/base/service.yaml</code> <code>metadata.annotations.metallb.universe.tf/loadBalancerIPs:</code> IP address IP Address from your MetalLB pool <p>For instructions on which values to use, see the <code>README.md</code> files in each of the workload directories.</p>"},{"location":"getting-started/installation/#helm-charts","title":"Helm Charts","text":"<p>Several of the infrastructure workloads are installed from Helm Charts. Go through each of the infrastructure workload directories, read their <code>README.md</code> files and follow their installation instructions.</p>"},{"location":"getting-started/repo-contents/","title":"Repository Contents","text":"<p>The repo contains:</p> <ul> <li><code>ansible/</code> and <code>ansible.cfg</code>, Ansible playbooks and configuration used for provisioning a fresh cluster and triggering bootstrap deployment of the cluster workloads, and cluster teardown.</li> <li><code>assets/</code>, containing images used in the documentation.</li> <li><code>changelog.d/</code> directory, containing changelog fragments and settings files for <code>scriv</code>, the changelog management tool for this repo.</li> <li>A <code>docs/</code> directory, containing the raw markdown files for the project documentation.</li> <li><code>bootstrap/</code>, <code>components/</code>, <code>infrastructure/</code> the GitOps directories for cluster bootstrap and ongoing management of the cluster configuration.</li> <li>A <code>tenants/</code> directory, which contains sample tenant applications to test your cluster is working.</li> <li>A <code>samples/</code> directory, to be used with Ansible to set up the inventory and values files. </li> <li>A <code>Makefile</code>, providing simple access to core commands necessary to provision, deploy and tear down the cluster.</li> <li>A <code>requirements.txt</code> Python requirements file listing requirements for development of this repo.</li> <li>A <code>mkdocs.yml</code> file, configuring Material for Mkdocs, the document generation tool for this repo. </li> <li><code>README.md</code>, the readme.</li> </ul> <p>And other various tool specific configuration files (google them).</p>"},{"location":"howtos/","title":"How-To Guides","text":"<p>These guides describe how to get specific tasks done. </p>"},{"location":"howtos/add-new-node/","title":"Adding new nodes to the cluster","text":"<p>Do this manually or tear down and re-provision the cluster with an updated inventory file. The Ansible playbooks are not designed to add new nodes to an existing cluster.</p> <p>Warning</p> <pre><code>All nodes need to be running the same version of k3s. So, if using the k3s installation script to set up a new node, watch out that it will install the latest stable k3s version, which may be more current than the versions currently running in the cluster. Upgrade the cluster, or install an older k3s version on the new node.\n</code></pre>"},{"location":"howtos/argocd-local-repo/","title":"How-To: Notes on using local git repos with ArgoCD","text":""},{"location":"howtos/argocd-local-repo/#dubious-authors","title":"Dubious authors","text":"<p>Try using a clean clone of a remote repo into the local copy on the host node, instead of pushing to the local copy on the host node.</p>"},{"location":"howtos/argocd-local-repo/#targetrevision","title":"<code>targetRevision</code>","text":"<p>This probably normally defaults to <code>HEAD</code>, but it can also be a branch name (this is undocumented in the argo docs). </p>"},{"location":"howtos/dynamic-environments/","title":"Dynamic environment promotion","text":"<p>TODO</p> <p>Use dynamic environments for staging. Create it with a namespace of its own, eg feature-a.staging.apps.cluster-name.domain.</p> <p>As of October 2023, there isn't a ArgoCD guide for how to manage environments with AppSets.</p>"},{"location":"howtos/helm-kustomize/","title":"Deploying Helm charts with ArgoCD and Kustomize","text":"<p>Resources:</p> <ul> <li>kustomize docs.</li> <li>Redhat article on helm kustomize methods.</li> <li>HariSekhon bug installing sealed-secrets with helm.</li> <li>Applying <code>--enable-helm</code> flag in ArgoCD config.</li> </ul> <p>Some workloads are deployed from Helm packages. There are multiple ways to do this with ArgoCD. This repo tries to use Kustomize for all deployment configuration management, including managing Helm packages. The workload is configured with the normal base/ overlays Kustomize pattern, with the Helm chart defined in the <code>base</code> <code>kustomization.yaml</code>. ArgoCD will then look after inflating the chart and patching it when it builds the ArgoCD <code>application</code>. </p> <p>To do this, ArgoCD must be told to use Helm in its internal Kustomize executable with the <code>kustomizeBuildOptions: \"--enable-helm\"</code> flag. This can be done by patching the <code>argocd-cm</code> <code>ConfigMap</code>.</p> <p>A <code>kustomization.yaml</code> set up in this method can be built locally to test it works with <code>kustomize build . --enable-helm</code>.</p>"},{"location":"howtos/kubectl/","title":"Kubectl","text":""},{"location":"howtos/kubectl/#accessing-the-kubeconfig-file","title":"Accessing the <code>kubeconfig</code> file","text":"<p><code>kubectl</code> is used to access the kubernetes API server on your cluster from the command line. Unless you previously installed <code>kubectl</code>, then after provisioning the K3s cluster, the <code>kubectl</code> command probably references the <code>kubectl</code> provided by K3s. Check this with <code>which kubectl</code>.</p> <p>By default, the K3s-provided <code>kubectl</code> references the cluster configuration provided at <code>/etc/rancher/K3s/K3s.yaml</code> which requires <code>sudo</code> privileges to run. This is deliberate, as it makes sure only users with superuser access privileges can manipulate the cluster. However, entering a sudo password quickly becomes annoying (convenience is the enemy of security).</p> <p>Instead, the <code>site</code> playbook drops the cluster config file into the default <code>kubectl</code> configuration directory at <code>~/.kube/config</code> (this is a file, not a directory). To get the K3s <code>kubectl</code> to reference this config file, either use the <code>--kubeconfig</code> flag or set the <code>KUBECONFIG</code> environment variable for your current shell.</p> <p>If you followed along in the Installation section, then you probably have <code>direnv</code> set up, which can be used to load the <code>KUBECONFIG</code> env variable automatically when you <code>cd</code> into your local copy of the GitOps directory:</p> <pre><code>echo \"export KUBECONFIG=$HOME/.kube/local-config\" &gt; .envrc\ndirenv allow\n</code></pre>"},{"location":"howtos/patching-argocd-cm/","title":"Example patch for <code>argocd-cm</code>","text":"<pre><code># patch-customdiff.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n\ndata:\n  resource.customizations: |\n      admissionregistration.k8s.io/ValidatingWebhookConfiguration:\n        ignoreDifferences: |\n          jsonPointers:\n            - /webhooks/0/clientConfig/caBundle  \n</code></pre>"},{"location":"reference/","title":"Reference","text":"<p>References are purely technical details. </p>"},{"location":"reference/#useful-links","title":"Useful links","text":"<ul> <li>Environment configuration best practice at Codefresh</li> <li>Kubernetes Deployment Anti-patterns</li> <li>Use syncwaves to impose order on separate activities.</li> </ul>"},{"location":"tutorials/","title":"Tutorial","text":"<p>In its initial state, this template deploys to a <code>dev</code> environment. The tutorials follow an incremental approach to eventually deploy to production:</p> <ul> <li>Start with a <code>dev</code> state for testing on your localhost with no Let's Encrypt interaction, then</li> <li>Deploys the <code>dev</code> configuration to all nodes in the cluster, then</li> <li>Deploys to a <code>stage</code> configuration to all nodes in the cluster to test syncing from a remote repo (GitHub) and certificate issuance from the Let's Encrypt staging API, then finally</li> <li>Deploys to a <code>prod</code> state on the whole cluster, which just changes to the production API on Let's Encrypt. </li> </ul> <p>Let's Encrypt has rate limiting on their production API, so it's good to make sure everything is working on staging first. The cluster can be torn down at any time if you wish to restart from a clean state.</p> <p>In summary:</p> Step State Nodes GitOps repo location <code>cert-manager</code> authority Part 01 <code>dev</code> <code>localhost</code> only Local filesystem on <code>localhost</code> with <code>hostPath</code> self-signed Part 02 <code>dev</code> All nodes Local filesystem on master node with <code>hostPath</code> self-signed Part 03 <code>stage</code> All nodes Remotely hosted repo Let's Encrypt staging API Part 04 <code>prod</code> All nodes Remotely hosted repo Let's Encrypt production API <p>Part 05 deploys a new app to the production cluster.</p>"},{"location":"tutorials/#activating-environments","title":"Activating environments","text":"<p>The presence of a <code>config.json</code> file in an environment directory of a workload is a marker to, and provides configuration for, ArgoCD, our GitOps deployment manager. This file tells Argo to deploy the kubernetes configuration present in that directory. Unused environments will have their <code>config.json</code> files renamed to <code>config.json.ignore</code>. </p> <p>The cluster configuration assumes that only one deployment environment is active for any one infrastructure workload at a time. If this rule is violated, you will probably experience networking conflicts.</p> <p>That's enough - go get started with Part 01.</p>"},{"location":"tutorials/tutorial01/","title":"Tutorial part 1: Localhost with <code>dev</code> infrastructure","text":"<p>This tutorial sets up a cluster for local testing so that you can get used to the deployment pattern and get the kubernetes config (yaml) working correctly, before having to worry about networking.</p> <p>For local testing, we will create a single-node K3s cluster on your local machine and deploy the <code>dev</code> environment. This option can be used to trial the repo before moving to a hosted GitOps repo, deploying to remote nodes, and/ or bringing in more than one node.</p> <p>First, make sure that you have completed the Installation. You may also benefit from reading about the Repo Contents before moving on, if you haven't already.</p> <p>Info</p> <p>The <code>K3s/first_server_argocd</code> role is skipped in the <code>site</code> playbook when deploying to a <code>localhost</code> cluster, provisioning just an empty cluster. The user is then responsible for deploying any workloads to that cluster manually, or with the provided <code>make local-bootstrap-argocd</code> command.</p>"},{"location":"tutorials/tutorial01/#create-local-git-repo","title":"Create local Git repo","text":"<p>To get started, create the local Git repository in your <code>/tmp</code> directory and add it as a new Git remote to your current Git working directory. Using the provided <code>make</code> convenience routine:</p> <pre><code>make local-create-repo\n</code></pre> <p>Tip</p> <p>Before running any <code>make</code> commands, you can make sure you're happy with them by opening up the <code>Makefile</code> in the project root and checking them first.</p> <p>The command assumes this directory is already configured as a Git repo (which will be the case if you cloned this from GitHub). This command creates a bare Git remote repository at <code>/tmp/argo-gitops.git</code> and adds it to the list of Git remotes under the short name <code>local-remote</code> in your current Git working directory (wherever you are running the <code>make</code> command from). </p> <p>You can add a remote repository manually if desired:</p> <pre><code>git remote add local-remote file:///tmp/argo-gitops.git\n</code></pre> <p>The <code>make local-create-repo</code> command does not push any files to the new remote. You still need to do this manually.</p> <p>You can now treat <code>/tmp/argo-gitops.git</code> like any other remote repository, allowing you to test how ArgoCD reconciles cluster state with GitOps repo state, including tracking different branches or commits, without having to upload anything to someone elses server.</p> <p>Note that by default, the ArgoCD ApplicationSets in <code>appset-infrastructure.yaml</code> and <code>appset-tenants.yaml</code> track the <code>main</code> branch, so you need to merge any changes into <code>main</code> in your working repository before pushing to the local remote or change which branch is tracked in those config files.</p> <p>Danger</p> <p>Note that the <code>reset</code> playbook and <code>make local-reset</code> both delete the <code>/tmp/argo-gitops.git</code> directory and all of its contents as part of cluster teardown. This local GitOps repo is for local testing only. The <code>/tmp</code> directory on a UNIX-like system is volatile, which means it is emptied whenever you reboot your machine. Do not rely on it for production!</p> <p>Verify the local remote has been created and added to your GitOps working repo:</p> <pre><code>ls /tmp/argo-gitops.git\ngit remote  # list remotes\ngit remote show local-remote # give details\n</code></pre> <p>Should see <code>local-remote</code> listed.</p> <p>To push from your working repo to the local remote:</p> <pre><code>git push local-remote main\n</code></pre>"},{"location":"tutorials/tutorial01/#set-the-deployment-environment","title":"Set the deployment environment","text":"<p>The cluster will by default deploy to a <code>dev</code> state. We will learn more about why in Part 2. For now, we just need to make sure all the infrastructure workloads are configured to deploy into environments compatible with <code>dev</code>. Make sure only the following ArgoCD generator marker files have the extension <code>*.json</code>. All other <code>config.json</code> files must be renamed to <code>config.json.ignore</code>. </p> <ul> <li><code>infrastructure/argocd/envs/dev/config.json</code></li> <li><code>infrastructure/longhorn/envs/prd/config.json</code></li> <li><code>infrastructure/traefik/envs/dev/config.json</code></li> </ul> <p>The other infrastructure workloads are always deployed into their <code>prod</code> environments.</p> <p>Note that the <code>cert-manager</code> authority that gets used to sign SSL certificates is dictated by <code>traefik</code>, which is why <code>cert-manager</code> can always be deployed to <code>prod</code>.</p>"},{"location":"tutorials/tutorial01/#provision-the-cluster","title":"Provision the cluster","text":"<p>Now provision the cluster to your local machine and deploy ArgoCD:</p> <pre><code>make local-up\n</code></pre> <p>This will execute the <code>site</code> playbook and:</p> <ul> <li>Execute the playbook against the <code>localhost.yaml</code> inventory file, which you should make sure lists connection details for the local machine in the <code>servers</code> group only, and no machines in the <code>agent</code> group.</li> <li>Provision a <code>localhost</code>-only cluster running K3s.</li> <li>Apply the bootstrap configuration files, which will deploy ArgoCD.</li> <li>Deploy the ApplicationSets, which will track the Git repo at <code>/tmp/argo-gitops.git</code> on your local machine.</li> </ul> <p>Now, each time you push updates on the <code>main</code> branch to that repo, ArgoCD will reconcile any differences between the configuration state in the repo and the live state in the cluster. This is the core of GitOps! Congratulations!</p> <p>The deployment may take 5 - 10 minutes worst case if it takes a while to download and deploy the containers.</p>"},{"location":"tutorials/tutorial01/#verification","title":"Verification","text":"<p>Check all pods are ready:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>If you get errors from <code>kubectl</code> suggesting it cannot find a <code>kubeconfig</code> file or similar, see Accessing the <code>kubeconfig</code> file.</p> <p>Now get the initial ArgoCD admin secret and forward the port for the web UI access to the argocd API:</p> <pre><code>make get-argocd-initpass\n</code></pre> <p>In your browser go to https://localhost:8080, login with user <code>admin</code>, and the password from above.</p> <p>Done! Note that you may need to give ArgoCD 2-5 minutes to reconcile and deploy the ApplicationSet (or force it yourself in the UI or CLI).</p>"},{"location":"tutorials/tutorial01/#generate-secrets","title":"Generate secrets","text":"<p>We need to generate sealed secrets for: </p> <ul> <li>Cert Manager DNS service access token. See Cert Manager workload readme.md.</li> <li>Traefik UI. See Traefik workload readme.</li> </ul>"},{"location":"tutorials/tutorial01/#other-useful-make-commands","title":"Other useful <code>make</code> commands","text":"<p>Some other <code>make</code> commands to ease local development:</p> <p>If you want to tear down the whole cluster:</p> <pre><code>make local-reset\n</code></pre> <p>Provision an empty K3s cluster to your local machine. That is, without also deploying the bootstrap deployment workloads:</p> <pre><code>make local-up-no-deploy\n</code></pre> <p>Deploy the bootstrap deployments to an existing local cluster from the local Git remote (ie. you must run <code>make local-create-repo</code> first and push something to it):</p> <pre><code>make local-deploy\n</code></pre> <p>Forward the port to the ArgoCD user interface. Useful if networking isn't working and you want to use the UI to debug Argo. This will make the ArgoCD login available at https://localhost:8080.</p> <pre><code>make fwd-argocd-server\n</code></pre>"},{"location":"tutorials/tutorial01/#cleanup","title":"Cleanup","text":"<p>Finally, tear down the local cluster ready for the next tutorial.</p> <pre><code>make local-reset\n</code></pre> <p>You can also use this command at any time if something went wrong and you want to start again. It triggers the <code>reset.yml</code> Ansible playbook which will stop the K3s executable, remove all installed files and directories from your machine, and undo any network configuration.</p>"},{"location":"tutorials/tutorial02/","title":"Tutorial part 2: Cluster with <code>dev</code> infrastructure","text":"<p>If not done already, follow the Installation instructions and make sure you have cleaned up any deployments you made in the first part of the tutorial.</p> <p>For all deployments to the cluster machines, we need to bootstrap the cluster first into the ArgoCD <code>dev</code> environment to get all of the infrastructure up and running. This is because we are using sealed-secrets to encrypt secrets, which needs to be first deployed and running on the cluster to create secrets to encrypt our remote Git (eg. GitHub) repo credentials. </p> <p>This means we need to create a local Git repo on the master server node that ArgoCD can pull config from, before we deploy the cluster. In this second tutorial, we will only create the GitOps repo on the master node and stop there. In Tutorial 3, we will update the cluster config, create the remote repo credential secrets and migrate to the remotely hosted repo as the source of truth for our cluster config.</p>"},{"location":"tutorials/tutorial02/#gitops-repo-on-master-server-node-local-filesystem","title":"GitOps repo on master server node local filesystem","text":"<p>Set up the Git remote repo on the local file system of the master server node. We will do this manually.</p> <p>Tip</p> <p>If you're doing your own thing and executing this tutorial on your local machine only, then you can use <code>make local-create-repo</code> for the next step, just as we did in Part 1.</p> <p>SSH into the master server node, then:</p> <pre><code>user@masterservernode:$ mkdir -p /tmp/argo-gitops.git\nuser@masterservernode:$ cd /tmp/argo-gitops.git \nuser@masterservernode:$ git init --bare\n</code></pre> <p>On your local workstation:</p> <pre><code>git remote add server-remote ssh://user@masterservernode/tmp/argo-gitops.git\n</code></pre> <p>Where <code>user@masterservernode</code> are the user and server address used to SSH into the master server node.</p> <p>Tip</p> <p>This assumes the ssh connection details for <code>user@masterservernode</code> are already available in <code>~/.ssh/config</code> on your local workstation, which allows specifying a non-standard port and an identity file. To go one step further and customise the ssh command itself, see Stackoverflow. </p> <p>For old (&lt;2.10) versions of Git, the <code>GIT_SSH_COMMAND</code> environment variable may also be useful. Git uses its own SSH client, sometimes it may be better to use the OpenSSH client instead, which can be done with this env variable.</p> <p>Push the <code>main</code> branch on your local workstation to the <code>server-remote</code> repo:</p> <pre><code>git push server-remote main\n</code></pre> <p>The GitOps repo is now ready to be used by the cluster.</p>"},{"location":"tutorials/tutorial02/#set-the-deployment-environment","title":"Set the deployment environment","text":"<p>Follow the same instructions as in Part 01. If you already followed the first tutorial, you shouldn't need to change any of the <code>config.json</code> files.</p>"},{"location":"tutorials/tutorial02/#deploy-the-cluster","title":"Deploy the cluster","text":"<p>We will now be deploying to the cluster nodes. Make sure <code>ansible/inventory/hosts.yml</code> is correctly populated. We will no longer be using <code>localhosts.yml</code> for the remainder of the tutorial.</p> <p>Get the site up:</p> <pre><code>make site-up\n</code></pre> <p>Once the playbook has finished, and if not done already, see Accessing the <code>kubeconfig</code> file.</p>"},{"location":"tutorials/tutorial02/#verification","title":"Verification","text":"<p>Check that all pods are ready:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>Get the initial ArgoCD admin secret:</p> <pre><code>make get-argocd-initpass\n</code></pre> <p>In your browser go to https://localhost:8080, login with user <code>admin</code>, and password from above.</p> <p>If you cannot access this URL, you may need to forward the port first:</p> <pre><code>make fwd-argocd-server\n</code></pre> <p>Congratulations, you're done! You can now test on your cluster using a locally hosted (to the cluster) repo.</p>"},{"location":"tutorials/tutorial02/#generate-secrets","title":"Generate secrets","text":"<p>If you want to test SSL certificate generation or access the Traefik UI, generate the required secrets.</p>"},{"location":"tutorials/tutorial02/#moving-on","title":"Moving on","text":"<p>In Part 3, we will repeat the above steps, but this time we will migrate the GitOps repo from the local Git repo to the remote repo. We will continue in Part 3 with the cluster in its current state. There is no need to tear it down.</p>"},{"location":"tutorials/tutorial02/#cleanup","title":"Cleanup","text":"<p>If you need to tear the cluster down at any time you can reset everything with:</p> <pre><code>make site-reset\n</code></pre> <p>Note that this will delete the remote repo at <code>/tmp/argo-gitops.git</code> on the master server node too. You will have to create it again if you reboot the master server node, or run <code>make site-reset</code> before you can deploy the cluster. </p>"},{"location":"tutorials/tutorial03/","title":"Tutorial part 3: Cluster with remote GitOps repo and <code>stage</code> infrastructure","text":"<p>This third part of the tutorial continues from the end of Part 2. If not done already, follow the Installation instructions, make sure you have cleaned up any deployments you made in Part 1, and set up the cluster as described in Part 2.</p>"},{"location":"tutorials/tutorial03/#deploy-the-cluster","title":"Deploy the cluster","text":"<p>If you just finished Part 2, then you should have the local Git repo on the master server node. To verify, get the ArgoCD password and log in to the UI as in the last tutorials to verify that everything is deployed correctly.</p>"},{"location":"tutorials/tutorial03/#configure-argocd-to-pull-from-the-remote-gitops-repo","title":"Configure ArgoCD to pull from the remote GitOps repo","text":"<p>If you followed the installation instructions, you will have forked the public instance of this GitOps repo into your own private GitOps repo. If you haven't already, push all of your local commits from Parts 1 and 2 of this tutorial to your remote repo:</p> <pre><code>git push origin main\n</code></pre> <p>We will now configure ArgoCD to use the remote repo as the source of truth for the cluster config. </p> <p>Find the <code>repoURL: https://github.com/USERNAME/YOUR-PRIVATE-FORK-OF-THIS-REPO.git</code> line in the following files and replace it with the URL to your private repo. You can get this URL by clicking on the \"Code\" button on the top right of the repo page on GitHub.</p> <ul> <li><code>components/envs/prod/patch-appset-infrastructure-generators.yaml</code></li> <li><code>components/envs/prod/patch-appset-infrastructure-source.yaml</code></li> <li><code>components/envs/prod/patch-appset-tenants-generators.yaml</code></li> <li><code>components/envs/prod/patch-appset-tenants-source.yaml</code></li> </ul> <p>Also set the private repo URL in the <code>sourceRepos:</code> field in <code>components/envs/prod/patch-appproj-dev1-sourceRepos.yaml</code>.</p> <p>These files configure where the cluster infrastructure and tenant workloads can be pulled from. At the moment, we are assuming that tenant workload configurations will be stored in the same repo as the infrastructure configurations. You can change this if you want in the <code>patch-appproj-dev1*</code> patch file and <code>patch-appset-tenants*.yaml</code> files listed above. These three files must have the same repo URL.</p>"},{"location":"tutorials/tutorial03/#create-remote-gitops-repo-secrets","title":"Create remote GitOps repo secrets","text":"<p>Repository details to be used with ArgoCD are stored in secrets. We will create a credential template so that the repository credentials can be re-used for any repo stored under your user profile on the remote host (eg. GitHub). The following assumes that you are using GitHub with an SSH connection. See ArgoCD docs on repositories for further information on other methods.</p>"},{"location":"tutorials/tutorial03/#using-github-personal-access-token-pat","title":"Using GitHub Personal Access Token (PAT)","text":"<p>This is the recommended method. It uses HTTPS to GitHub and a PAT for authentication.</p> <p>Resources:</p> <ul> <li>RedHat.</li> </ul> <p>Create a GitHub Personal Access Token (PAT) for your private repo. </p> <p>Create the plain text secret:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: gh-private-k3s-pat\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  url: https://github.com/USERNAME/REPO\n  password: Paste the PAT here\n  username: not-used\n</code></pre> <p>Then seal the secret:</p> <pre><code>kubeseal --format=yaml --cert=pub-sealed-secrets.pem --secret-file components/envs/prod/secret-private-gh-infra-repository-pat.yaml.pass --sealed-secret-file components/envs/prod/secret-private-gh-infra-repository-pat-sealed.yaml\n</code></pre> <p>Add it to kustomize:</p> <pre><code># components/envs/prod/kustomization.yaml\n\nresources:\n  - secret-private-gh-infra-repository-pat-sealed.yaml\n</code></pre> <p>Make sure the argo labels are retained in the sealed secret yaml:</p> <pre><code># secret-private-gh-infra-repository-pat-sealed.yaml\n...\nkind: SealedSecret\n...\nspec:\n  template:\n    metadata:\n      labels: \n        \"argocd.argoproj.io/secret-type\": repository\n...\n</code></pre>"},{"location":"tutorials/tutorial03/#using-ssh","title":"Using SSH","text":"<p>Failure</p> <p>Could not get this to work with a private GitHub repo over ssh. The repo would not authenticate.</p> <p>Note</p> <p>ArgoCD does not support SSH private keys protected with a passphrase. See issue#1894.</p> <p>In your local repo:</p> <pre><code>ssh-keygen -t ed25519 -C \"your_email@example.com\"  # your github account email\n\n# Save to a file \"argocd-to-gh\"\n# Leave the passphrase empty\n</code></pre> <p>We won't be using this private key on a local machine and it isn't protected with a passphrase, so we do not need to add it to any key chain or <code>ssh-agent</code>.</p> <p>Copy the public key to your GitHub account.</p> <p>Create local plain text secrets</p> <p>Sealed Secrets works by taking a Kubernetes secret object and encrypting it with a public key provided by the Sealed Secrets application (which is why the cluster must be deployed first, so the Sealed Secrets is available to perform this operation). </p> <p>We next create a <code>yaml</code> file to describe the secret object, with the secret stored in plain text on our local file system only. We will then encrypt it using Sealed Secrets and output it to an encrypted <code>yaml</code> file. This encrypted file can safely be uploaded to a Git repository. The only entity that can now unencrypt the secret is the Sealed Secrets instance running inside your cluster. Not even the author of the secret can unencrypt it. </p> <p>Tip</p> <p>As a matter of convention, you can name your plaintext secret files with the extension <code>*.pass</code>, then add <code>*.pass</code> as an entry to your <code>.gitignore</code> file to make sure plaintext files do not get uploaded to remote repositories. </p> <p>First, create the plain text credential template secret object:</p> <pre><code># components/envs/prod/secret-private-gh-credentials.yaml.pass\napiVersion: v1\nkind: Secret\nmetadata:\n  name: private-gh-creds\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repo-creds\nstringData:\n  type: git\n  url: https://github.com:USERNAME\n  sshPrivatekey: |\n    -----BEGIN OPENSSH PRIVATE KEY-----  #gitleaks:allow\n    ...\n    -----END OPENSSH PRIVATE KEY-----  #gitleaks:allow\n</code></pre> <p>(Ignore the gitleaks tags - these are to prevent false positives when running gitleaks on the repo).</p> <p>Open the private key file <code>id_ed25519-argocd</code> and copy the private key contents into the appropriate section in the above credentials secret.</p> <p>Now create the plain text repository secret object:</p> <pre><code># components/envs/prod/secret-private-gh-infra-repository.yaml.pass\napiVersion: v1\nkind: Secret\nmetadata:\n  name: private-infra-repo\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/USERNAME/YOUR-PRIVATE-FORK-OF-THIS-REPO.git\n</code></pre> <p>Replace the URLs with your correct username and repo address. </p> <p>Encrypt secrets</p> <p>Fetch the public key from Sealed secrets. In the root directory of your GitOps repo:</p> <pre><code>kubeseal --fetch-cert &gt; pub-sealed-secrets.pem\n</code></pre> <p>Tip</p> <p>This public key can be shared in your GitOps repo with no risk to security. However, if you don't want to share it, and <code>*.pem</code> to your <code>.gitignore</code> file.</p> <p>We can now use the public key to encrypt our secrets. From the project root directory:</p> <pre><code>kubeseal --format=yaml \\\n  --cert=pub-sealed-secrets.pem \\\n  --secret-file components/envs/prod/secret-private-gh-credentials.yaml.pass \\\n  --sealed-secret-file components/envs/prod/secret-private-gh-credentials-sealed.yaml\n\nkubeseal --format=yaml \\\n  --cert=pub-sealed-secrets.pem \\\n  --secret-file components/envs/prod/secret-private-gh-infra-repository.yaml.pass \\\n  --sealed-secret-file components/envs/prod/secret-private-gh-infra-repository-sealed.yaml\n</code></pre> <p>As the ArgoCD docs note, Sealed Secrets will remove the labels from the secret object we defined in the plain text file. We need to readd them manually. Add the following to the sealed yaml files:</p> <pre><code># secret-private-gh-credentials-sealed.yaml\n...\nkind: SealedSecret\n...\nspec:\n  template:\n    metadata:\n      labels: \n        \"argocd.argoproj.io/secret-type\": repo-creds\n...\n</code></pre> <pre><code># secret-private-gh-infra-repository-sealed.yaml\n...\nkind: SealedSecret\n...\nspec:\n  template:\n    metadata:\n      labels: \n        \"argocd.argoproj.io/secret-type\": repository\n...\n</code></pre> <p>Add the secret files to <code>kustomization.yaml</code>:</p> <pre><code># components/envs/prod/kustomization.yaml\n...\nresources:\n  - secret-private-gh-infra-repository-sealed.yaml\n  - secret-private-gh-credentials-sealed.yaml\n  - ...\n</code></pre> <p>You can delete the <code>*.pass</code> files now if you wish, but you will need to recreate them to re-encrypt the secrets if you want to update them later.</p> <p>Warning</p> <p>If using SSH and a custom Git repository, you will also need to add SSH known host public keys to ArgoCD. Argo already has the GitHub (and some others) known host keys built in, so in this case you don't need to do anything extra. See the docs.</p> <p>This should be all the configuration changes we need.</p> <p>You can now delete the public and private keys - they aren't needed anymore (the private key is stored in your cluster in a SealedSecret, and the public key is copied to GitHub).</p>"},{"location":"tutorials/tutorial03/#push-the-new-config-to-the-cluster","title":"Push the new config to the cluster","text":"<pre><code># commit the new secrets to the gitops repo\ngit push server-remote main\n</code></pre> <p>Refresh the argo app in the UI. You wont see the new secrets in the cluster yet because we are still running ArgoCD in its <code>dev</code> environment. </p>"},{"location":"tutorials/tutorial03/#change-argocd-applictionset-files-to-remote-repo","title":"Change ArgoCD applictionSet files to remote repo","text":"<p>Switch to the remote repo patch files in <code>components/envs/prod/kustomization.yaml</code>:</p> <pre><code>patches:\n# use these for remote gitops repo\n  - path: patch-appproj-dev1-sourceRepos.yaml\n  - path: patch-appset-infrastructure-generators.yaml\n  - path: patch-appset-infrastructure-source.yaml\n  - path: patch-appset-tenants-generators.yaml\n  - path: patch-appset-tenants-source.yaml\n</code></pre>"},{"location":"tutorials/tutorial03/#migrate-to-remote-gitops-repo","title":"Migrate to remote GitOps repo","text":"<p>Commit the above changes and push them to the remote repo. Then push the changes to the local repo on the master node.</p> <pre><code>git push origin main\ngit push server-remote main\n</code></pre> <p>ArgoCD polls the repo every three minutes. If you can't wait that long, click <code>Sync</code> on the ArgoCD app in the UI. Ideally, nothing should visibly change. Check which repo is being polled in Settings&gt;Repositories. Congratulations!</p> <p>You now no longer need to push any code to the local repo on the master server node. Everything is on the remote repo and you can benefit from all the reliability that comes from the GitHub servers. </p>"},{"location":"tutorials/tutorial03/#migrate-to-lets-encrypt-staging-api","title":"Migrate to Let's Encrypt staging API","text":"<p>Finally, we can migrate the SSL certificates from self-signed to the Let's Encrypt staging API. This will let us test the SSL configuration without using the Let's Encrypt production API, which is rate limited.</p> <p>Rename the ArgoCD marker files to:</p> <ul> <li><code>infrastructure/traefik/envs/dev/config.json.ignore</code></li> <li><code>infrastructure/traefik/envs/stage/config.json</code></li> </ul> <p>After letting ArgoCD sync the changes (or syncing manually via the UI), you should see Traefik move to the new environment. You may have to wait 5 - 10 minutes for the staging certificate to be registered and received from Let's Encrypt. Once it has loaded, you can visit the websites of your workloads with <code>https://</code> and inspect the certificates. </p> <p>Note that the staging certificates will still give an error in your browser as they are not signed. But we can open the certificates up from the browser and check that they are being issued from the Let's Encrypt staging endpoint, which tells us we can safely move on to the Let's Encrypt production API.</p> <p>Warning</p> <p>If you haven't done it already, you will need to generate a secret for the Let's Encrypt DNS service access token. See generate the required secrets in Part 1 of this tutorial.</p>"},{"location":"tutorials/tutorial03/#moving-on","title":"Moving On","text":"<p>In the next part of the tutorial, we will migrate the cluster to a fully <code>prod</code> environment. Get started on Part 4.</p>"},{"location":"tutorials/tutorial04/","title":"Tutorial part 4: Cluster with remote GitOps repo and <code>prod</code> infrastructure","text":"<p>This fourth part of the tutorial continues from the end of Part 3. If not done already, follow the Installation instructions, make sure you have cleaned up any deployments you made in Part 1, and set up the cluster as described in Parts 2 and 3.</p> <p>You should have a cluster deployed to your node machines with ArgoCD monitoring your private external repo for changes to configuration, and SSl certificates being served by the Let's Encrypt staging endpoint.</p> <p>If not done already, follow the Installation instructions.</p>"},{"location":"tutorials/tutorial04/#checks-prior-to-launching-prod","title":"Checks prior to launching <code>prod</code>","text":"<ul> <li>Remove the default ArgoCD admin password and set your own. This is possible through the ArgoCD UI.</li> <li>Ensure the Traefik dashboard has a password.</li> <li>Optional: Set up backups and snapshots in Longhorn. See the Longhorn docs.</li> </ul>"},{"location":"tutorials/tutorial04/#change-all-environments-to-prod","title":"Change all environments to <code>prod</code>","text":"<p>Ensure the following ArgoCD marker files are renamed, and any sibling environment marker files are renamed to <code>config.json.ignore</code>:</p> <ul> <li><code>infrastructure/argocd/envs/prod/config.json</code></li> <li><code>infrastructure/cert-manager/prod/config.json</code></li> <li><code>infrastructure/kubevip/envs/prod/config.json</code></li> <li><code>infrastructure/longhorn/envs/prod/config.json</code></li> <li><code>infrastructure/metallb/envs/prod/config.json</code></li> <li><code>infrastructure/sealed-secrets/envs/prod/config.json</code></li> <li><code>infrastructure/traefik/envs/stage/config.json</code></li> </ul> <p>As well as the test tenant applications:</p> <ul> <li><code>tenants/test-ingress/envs/prod/config.json</code></li> <li><code>tenants/test-lb/envs/prod/config.json</code></li> </ul> <p>Commit and push the changes to the private GitOps repo.</p> <p>It should migrate within 3 minutes, or manually sync the applications in the ArgoCD UI. You should no longer see certificate errors when visiting <code>https://</code> page hosted by the cluster.</p>"},{"location":"tutorials/tutorial04/#congratulations","title":"Congratulations!","text":"<p>You've deployed the cluster to production. Now you can deploy your own tenant workloads to the cluster.</p>"},{"location":"tutorials/tutorial05/","title":"Tutorial part 5: Deploy new app to cluster with <code>prod</code> infrastructure","text":"<p>TODO</p> <p>App version shall be defined in a yaml, with no other config in that yaml. This allows it to be easily bumped and diffed, and copied over.</p> <p>Note that a namespace with the pattern <code>&lt;appname&gt;-&lt;environment&gt;</code>, using the exact same wording as used in the directory structure under <code>tenants/</code> must be set up by the cluster admin in <code>components/namespaces</code>before you can deploy the app.</p>"},{"location":"blog/","title":"Blog","text":""}]}